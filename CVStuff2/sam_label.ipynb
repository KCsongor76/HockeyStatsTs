{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-18T20:24:35.583468Z"
    }
   },
   "source": [
    "รง# Install PyTorch (ensure CUDA support for video processing)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "# Install SAM 3 (Hypothetical repo based on Nov 2025 release)\n",
    "!pip install git+https://github.com/facebookresearch/sam3.git\n",
    "\n",
    "# Install OpenCV and Matplotlib for visualization\n",
    "!pip install opencv-python matplotlib"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\r\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement torch (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for torch\u001B[0m\u001B[31m\r\n",
      "\u001B[0mCollecting git+https://github.com/facebookresearch/sam3.git\r\n",
      "  Cloning https://github.com/facebookresearch/sam3.git to /private/var/folders/7_/ppvl_3r15_5fhytbh4trswq40000gn/T/pip-req-build-d48ovl1h\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/sam3.git /private/var/folders/7_/ppvl_3r15_5fhytbh4trswq40000gn/T/pip-req-build-d48ovl1h\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0mCollecting opencv-python\r\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_x86_64.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: matplotlib in /Users/kcsongor/IT/Anaconda/anaconda3/lib/python3.12/site-packages (3.9.2)\r\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\r\n",
      "  Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/kcsongor/IT/Anaconda/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kcsongor/IT/Anaconda/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kcsongor/IT/Anaconda/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kcsongor/IT/Anaconda/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kcsongor/IT/Anaconda/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/kcsongor/IT/Anaconda/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kcsongor/IT/Anaconda/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kcsongor/IT/Anaconda/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "INFO: pip is looking at multiple versions of contourpy to determine which version is compatible with other requirements. This could take a while.\r\n",
      "\u001B[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/contourpy/\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting contourpy>=1.0.1 (from matplotlib)\r\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sam3 import build_sam3, SAM3Predictor\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "VIDEO_PATH = \"assets/game_footage_01.mp4\"\n",
    "OUTPUT_DIR = \"training_data/labels\"\n",
    "IMAGES_DIR = \"training_data/images\"\n",
    "# Map text prompts to YOLO Class IDs\n",
    "CLASS_MAP = {\n",
    "    \"hockey player\": 0,\n",
    "    \"puck\": 1,\n",
    "    \"referee\": 2\n",
    "}\n",
    "\n",
    "# --- MODEL LOADING ---\n",
    "# Load the 'Large' model for maximum accuracy during offline auto-labeling\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Loading SAM 3 on {device}...\")\n",
    "sam3_model = build_sam3(checkpoint=\"checkpoints/sam3_large.pth\").to(device)\n",
    "predictor = SAM3Predictor(sam3_model)\n",
    "\n",
    "def masks_to_yolo_boxes(masks_np):\n",
    "    \"\"\"\n",
    "    Vectorized conversion of binary masks to YOLO (x_center, y_center, w, h) format.\n",
    "\n",
    "    Args:\n",
    "        masks_np (np.ndarray): Boolean array of shape (N, H, W)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N, 4) with normalized YOLO coordinates\n",
    "    \"\"\"\n",
    "    n, h, w = masks_np.shape\n",
    "    if n == 0:\n",
    "        return np.empty((0, 4))\n",
    "\n",
    "    boxes = []\n",
    "\n",
    "    # Vectorized bounding box extraction\n",
    "    # We project the mask onto X and Y axes to find min/max\n",
    "    for i in range(n):\n",
    "        mask = masks_np[i]\n",
    "        if not np.any(mask):\n",
    "            boxes.append([0, 0, 0, 0]) # Placeholder for empty\n",
    "            continue\n",
    "\n",
    "        rows = np.any(mask, axis=1)\n",
    "        cols = np.any(mask, axis=0)\n",
    "        y_min, y_max = np.where(rows)[0][[0, -1]]\n",
    "        x_min, x_max = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "        # Calculate width/height in pixels\n",
    "        w_pixel = x_max - x_min\n",
    "        h_pixel = y_max - y_min\n",
    "\n",
    "        # Normalize (YOLO format: center_x, center_y, w, h)\n",
    "        x_center = (x_min + w_pixel / 2) / w\n",
    "        y_center = (y_min + h_pixel / 2) / h\n",
    "        w_norm = w_pixel / w\n",
    "        h_norm = h_pixel / h\n",
    "\n",
    "        boxes.append([x_center, y_center, w_norm, h_norm])\n",
    "\n",
    "    return np.array(boxes)\n",
    "\n",
    "def generate_training_data(video_path, class_map):\n",
    "    # 1. Initialize Video State\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    inference_state = predictor.init_state(video_path=video_path)\n",
    "\n",
    "    print(f\"Processing {total_frames} frames for classes: {list(class_map.keys())}\")\n",
    "\n",
    "    # 2. PROMPT: Text-based concept tracking (The SAM 3 Magic)\n",
    "    # We pass the list of text prompts. SAM 3 tracks them across the video.\n",
    "    prompts = list(class_map.keys())\n",
    "    # Note: 'batch_size' controls VRAM usage.\n",
    "    video_output = predictor.propagate_in_video(\n",
    "        inference_state,\n",
    "        text_prompts=prompts,\n",
    "        batch_size=8\n",
    "    )\n",
    "\n",
    "    # 3. Serialize Data\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    Path(IMAGES_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    current_frame = 0\n",
    "\n",
    "    # We iterate through the generator or result dict from SAM 3\n",
    "    for frame_idx, frame_data in tqdm(video_output.items(), total=total_frames):\n",
    "\n",
    "        # Setup Label File\n",
    "        label_path = os.path.join(OUTPUT_DIR, f\"{Path(video_path).stem}_{frame_idx:06d}.txt\")\n",
    "\n",
    "        # Read frame (only needed if you want to save the JPGs for training)\n",
    "        ret, frame_img = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        # Save Frame Image (YOLO requires image + label pair)\n",
    "        image_path = os.path.join(IMAGES_DIR, f\"{Path(video_path).stem}_{frame_idx:06d}.jpg\")\n",
    "        cv2.imwrite(image_path, frame_img)\n",
    "\n",
    "        labels_str = []\n",
    "\n",
    "        # frame_data maps { prompt_text: masks_tensor } or similar structure\n",
    "        for text_prompt, masks_logits in frame_data.items():\n",
    "            class_id = class_map[text_prompt]\n",
    "\n",
    "            # Binarize masks\n",
    "            masks_binary = (masks_logits > 0.0).cpu().numpy().squeeze()\n",
    "\n",
    "            # Handle single vs batch dimension\n",
    "            if masks_binary.ndim == 2:\n",
    "                masks_binary = masks_binary[np.newaxis, ...]\n",
    "\n",
    "            # Get boxes\n",
    "            yolo_boxes = masks_to_yolo_boxes(masks_binary)\n",
    "\n",
    "            for box in yolo_boxes:\n",
    "                # Filter noise (e.g. 0-area boxes)\n",
    "                if box[2] < 0.001 or box[3] < 0.001: continue\n",
    "\n",
    "                # Format: class x y w h\n",
    "                labels_str.append(f\"{class_id} {box[0]:.6f} {box[1]:.6f} {box[2]:.6f} {box[3]:.6f}\")\n",
    "\n",
    "        # Write Label File\n",
    "        if labels_str:\n",
    "            with open(label_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(labels_str))\n",
    "\n",
    "    cap.release()\n",
    "    print(\"Auto-labeling complete.\")\n",
    "\n",
    "# --- EXECUTE ---\n",
    "if __name__ == \"__main__\":\n",
    "    generate_training_data(VIDEO_PATH, CLASS_MAP)"
   ],
   "id": "496f5b95135caa42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_sample(image_dir, label_dir, num_samples=3):\n",
    "    image_files = sorted(list(Path(image_dir).glob(\"*.jpg\")))[:num_samples]\n",
    "\n",
    "    for img_path in image_files:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        label_path = Path(label_dir) / f\"{img_path.stem}.txt\"\n",
    "        if not label_path.exists(): continue\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            cls, xc, yc, wn, hn = map(float, line.strip().split())\n",
    "\n",
    "            # Denormalize\n",
    "            x1 = int((xc - wn/2) * w)\n",
    "            y1 = int((yc - hn/2) * h)\n",
    "            x2 = int((xc + wn/2) * w)\n",
    "            y2 = int((yc + hn/2) * h)\n",
    "\n",
    "            color = (0, 255, 0) if cls == 0 else (255, 0, 0) # Green for player, Red for others\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Sample: {img_path.name}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "visualize_sample(IMAGES_DIR, OUTPUT_DIR)"
   ],
   "id": "3126d57062cee514"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
